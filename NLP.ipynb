{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikBL1nnRy6mH"
      },
      "source": [
        "# Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwSXu-Rg6FMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070d1704-e541-4b24-a7f4-a4047fcd0795"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJEqjzvdzM25"
      },
      "source": [
        "sentence = \"the boy ate pancakes at the restaurant\""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLH4xT9jzTzN",
        "outputId": "9851197c-d9dd-49a1-b4e1-7cbc8e1ee9bd"
      },
      "source": [
        "token = nltk.word_tokenize(sentence)\n",
        "token"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'boy', 'ate', 'pancakes', 'at', 'the', 'restaurant']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNT_0ugMzbeO",
        "outputId": "b42520b5-2d85-4d33-99d4-14591456f9cf"
      },
      "source": [
        "tags = nltk.pos_tag(token)\n",
        "tags"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 'DT'),\n",
              " ('boy', 'NN'),\n",
              " ('ate', 'NN'),\n",
              " ('pancakes', 'NNS'),\n",
              " ('at', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('restaurant', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn5vfGHz6JUw"
      },
      "source": [
        "# Expand Clitics/Contractions (eg: She's to She is)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsoTJjgYzmL2",
        "outputId": "064eac55-febc-4986-f95f-876f74f8c3aa"
      },
      "source": [
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "statement = \"She's going for a walk. Jack didn't go because he was tired. They'll surely go for a walk together tomorrow.\"\n",
        "\n",
        "expanded_statement = []\n",
        "\n",
        "for word in statement.split():\n",
        "  expanded_statement.append(contractions.fix(word))\n",
        "\n",
        "fixed_statement = \" \".join(expanded_statement)\n",
        "\n",
        "print(fixed_statement)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.58)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "she is going for a walk. Jack did not go because he was tired. they will surely go for a walk together tomorrow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRIk2Rpjrd38"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ct7CQC7D4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96176b0d-695a-4c6f-dab5-a5cb1049036d"
      },
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rLtfiirroz4",
        "outputId": "86d0642e-2420-4e9c-af13-e3c91d0b3d8e"
      },
      "source": [
        "sentence = \"He's a German Shepherd. They are some of the smartest dogs!\"\n",
        "t = word_tokenize(sentence)\n",
        "t"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He',\n",
              " \"'s\",\n",
              " 'a',\n",
              " 'German',\n",
              " 'Shepherd',\n",
              " '.',\n",
              " 'They',\n",
              " 'are',\n",
              " 'some',\n",
              " 'of',\n",
              " 'the',\n",
              " 'smartest',\n",
              " 'dogs',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-sQLr25r-bX",
        "outputId": "3fc8ab6b-902d-4ae7-f41e-290e688c88b5"
      },
      "source": [
        "multiple = \"Hi! My name is Sid. I live in Mumbai, India.\"\n",
        "x = sent_tokenize(multiple)\n",
        "x"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi!', 'My name is Sid.', 'I live in Mumbai, India.']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrHR8cxvkyT3"
      },
      "source": [
        "# Use contrations library to expand clitics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDTlhBk0sM7A",
        "outputId": "dcb24e49-0177-4ab0-ede2-e8b987f00cf3"
      },
      "source": [
        "import contractions\n",
        "\n",
        "a = contractions.fix(\"you're happy now\")\n",
        "b = contractions.fix(\"yall're happy now.\", slang = False)\n",
        "c = contractions.fix(\"yall're happy now.\")\n",
        "\n",
        "a, b, c"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('you are happy now', \"yall're happy now.\", 'you all are happy now.')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvT3Dc-pl4gL"
      },
      "source": [
        "# REGEX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocCKXR_klM2H",
        "outputId": "b9e57a6a-df1c-4d8c-b806-ac0fdd0a0f75"
      },
      "source": [
        "sentence = \"He's fast. The buy can run up the hill in a jiffy!\"\n",
        "\n",
        "result = sentence.split()\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"He's\", 'fast.', 'The', 'buy', 'can', 'run', 'up', 'the', 'hill', 'in', 'a', 'jiffy!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RVAirr4mQou",
        "outputId": "be40d57e-ea80-428c-a7fb-fde67e194319"
      },
      "source": [
        "import re\n",
        "\n",
        "text = \"uno-dos+tres#quatro cinco\"\n",
        "\n",
        "x = re.split(\"\\W+\", text)\n",
        "\n",
        "x"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uno', 'dos', 'tres', 'quatro', 'cinco']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leBzNzs2nLOW",
        "outputId": "cc7e2c19-e7bf-45c1-d0be-b39461095059"
      },
      "source": [
        "# alternate approach\n",
        "\n",
        "y = re.split(\"[-+#]\", text)\n",
        "\n",
        "y"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uno', 'dos', 'tres', 'quatro cinco']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ckYnKmMepIpH",
        "outputId": "b83c9b60-a89b-47b3-8ea3-b9fdcf332bd3"
      },
      "source": [
        "# re.sub()\n",
        "\n",
        "string = \"Hi, I am currently in New York.\"\n",
        "\n",
        "new_string = re.sub(\"New York\", \"Mumbai\", string)\n",
        "\n",
        "new_string"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hi, I am currently in Mumbai.'"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km8F5KMRqGwX",
        "outputId": "988bb7b3-9530-4755-b69f-1bbc67c02d66"
      },
      "source": [
        "words = [\"Sid\", \"Mumbai\", \"Bangalore\"]\n",
        "\n",
        "statement = \"Hi! My name is Sid. I live in Mumbai, India.\"\n",
        "\n",
        "for word in words:\n",
        "  print(\"Looking for '{}' in '{}': \\n\".format(word, statement))\n",
        "\n",
        "  if re.search(word, statement):\n",
        "    print(\"--search succesful--\\n\")\n",
        "  else:\n",
        "    print(\"--word not found--\\n\")\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for 'Sid' in 'Hi! My name is Sid. I live in Mumbai, India.': \n",
            "\n",
            "--search succesful--\n",
            "\n",
            "Looking for 'Mumbai' in 'Hi! My name is Sid. I live in Mumbai, India.': \n",
            "\n",
            "--search succesful--\n",
            "\n",
            "Looking for 'Bangalore' in 'Hi! My name is Sid. I live in Mumbai, India.': \n",
            "\n",
            "--word not found--\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBlA5Ei13pIJ",
        "outputId": "e3666530-1255-4ddc-d108-a2df489371ba"
      },
      "source": [
        "# re.findall()\n",
        "\n",
        "\n",
        "contact = \"Sid Krishnan; sid@sidkrishnan.com; Mumbai, India;\"\n",
        "\n",
        "email = re.findall(r\"[\\w\\.\\-]+@[.\\w\\-]+\", contact)\n",
        "\n",
        "email"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sid@sidkrishnan.com']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rpzmeBu4zVf",
        "outputId": "b41562b6-7ac8-473f-b903-2e25b4b6454c"
      },
      "source": [
        "# Context-Free Grammar Rules\n",
        "\n",
        "# parse tree\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt') # punkt is a pre-trained tokenizer\n",
        "nltk.download('averaged_preceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "\n",
        "sentence = \"I want a morning flight.\""
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Error loading averaged_preceptron_tagger: Package\n",
            "[nltk_data]     'averaged_preceptron_tagger' not found in index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abnH2KtIMBTZ",
        "outputId": "91e1533e-029f-4159-8e9d-d37fcbab1483"
      },
      "source": [
        "tags = pos_tag(word_tokenize(sentence))\n",
        "\n",
        "tags"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('want', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('morning', 'NN'),\n",
              " ('flight', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl3lRPV5PMT3"
      },
      "source": [
        "# chunk extraction\n",
        "\n",
        "chunker = RegexpParser(\"\"\"\n",
        "                       NP : {<DT>?<JJ>*<NN>}        # to extract noun phrase\n",
        "                       P : {<IN>}                   # to extract prepositions\n",
        "                       V : {<V.*>}                  # to extract verbs\n",
        "                       PP : {<P> <NP>}              # to extract preposition phrase\n",
        "                       VP : {<V> <NP|PP>*}          # to extract verb phrase\n",
        "                       \"\"\")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0evjKYARC2i",
        "outputId": "14b6782a-b68d-4194-e664-4b8b71074d0c"
      },
      "source": [
        "result = chunker.parse(tags)\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S I/PRP (VP (V want/VBP) (NP a/DT morning/NN) (NP flight/NN)) ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PatOJn0sRIOP",
        "outputId": "4ba2a32f-2b81-4268-a6f2-39afce7819df"
      },
      "source": [
        "statement = \"The boy ate the pancakes from the restaurant\"\n",
        "\n",
        "tags = pos_tag(word_tokenize(statement))\n",
        "result = chunker.parse(tags)\n",
        "print(result)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT boy/NN)\n",
            "  (VP (V ate/VB))\n",
            "  the/DT\n",
            "  pancakes/NNS\n",
            "  (PP (P from/IN) (NP the/DT restaurant/NN)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL7jPC31SLIo"
      },
      "source": [
        "# result.draw() --- this doesnt work on colab but does work on jupyter"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3-zleHeTInC"
      },
      "source": [
        "# STEMMING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es8FJGAXSVMv"
      },
      "source": [
        "# 3 main methods:\n",
        "\n",
        " # -- Porter \n",
        " # -- Lancaster\n",
        " # -- Snowball\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjvNQ9z-U32x",
        "outputId": "844072ed-f59b-4888-cec3-1cb318f1950a"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "print(porter.stem(\"running\"))\n",
        "print(porter.stem(\"bundle\"))\n",
        "print(porter.stem(\"illustrator\"))\n",
        "print(porter.stem(\"slept\"))\n",
        "print(porter.stem(\"restaurant\"))\n",
        "print(porter.stem(\"organization\"))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "bundl\n",
            "illustr\n",
            "slept\n",
            "restaur\n",
            "organ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZqRvjWHVDmb",
        "outputId": "57bc2600-6f8f-488b-ef20-876539ede93e"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "lanc = LancasterStemmer()\n",
        "\n",
        "print(lanc.stem(\"running\"))\n",
        "print(lanc.stem(\"bundle\"))\n",
        "print(lanc.stem(\"illustrator\"))\n",
        "print(lanc.stem(\"slept\"))\n",
        "print(lanc.stem(\"restaurant\"))\n",
        "print(lanc.stem(\"organization\"))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "bundl\n",
            "illust\n",
            "slept\n",
            "resta\n",
            "org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc3bAgp7VnAD",
        "outputId": "46dd24a8-8a42-4f61-c12f-06a3e9878e1b"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snow = SnowballStemmer(\"english\")\n",
        "\n",
        "print(snow.stem(\"running\"))\n",
        "print(snow.stem(\"bundle\"))\n",
        "print(snow.stem(\"illustrator\"))\n",
        "print(snow.stem(\"slept\"))\n",
        "print(snow.stem(\"restaurant\"))\n",
        "print(snow.stem(\"organization\"))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "bundl\n",
            "illustr\n",
            "slept\n",
            "restaur\n",
            "organ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hdAHSEYaS_L"
      },
      "source": [
        "# LEMMATIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWyCL0hIV-JZ",
        "outputId": "560e7672-b435-410f-c1e5-da030d9ca59a"
      },
      "source": [
        "import nltk \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm5VF36Zbkgq",
        "outputId": "9e1ebb7c-d891-40c5-b5b7-f2b2fda5ddae"
      },
      "source": [
        "lem = WordNetLemmatizer()\n",
        "token = \"spies\"\n",
        "token2 = \"festivities\"\n",
        "\n",
        "lemma = lem.lemmatize(token)\n",
        "lemma2 = lem.lemmatize(token2)\n",
        "\n",
        "print(token, \" --> \", lemma)\n",
        "print(token2, \" --> \", lemma2)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spies  -->  spy\n",
            "festivities  -->  festivity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQULT7Cjc4_r",
        "outputId": "32770250-d9d0-47d7-818e-e9c2a0351fe7"
      },
      "source": [
        "string1 = \"Mumbai is India's commercial capital\"\n",
        "string2 = \"My favourite part of Mumbai is Marine Drive\"\n",
        "\n",
        "tokens = nltk.word_tokenize(string1)\n",
        "print(\"Tokenized sentence : \", tokens)\n",
        "\n",
        "# lemmatize the tokenized sentence:\n",
        "lemmatized_tokens = \" \".join(lem.lemmatize(x) for x in tokens)\n",
        "print(\"Lemmatized tokens : \", lemmatized_tokens)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentence :  ['Mumbai', 'is', 'India', \"'s\", 'commercial', 'capital']\n",
            "Lemmatized tokens :  Mumbai is India 's commercial capital\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si_0oG-VMIVP"
      },
      "source": [
        "# PRACTICAL EXAMPLES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBg8HZQBML0U"
      },
      "source": [
        "# Clean Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81OlWW1p1UPJ"
      },
      "source": [
        "def hashtags(tweet):\n",
        "  hash = re.findall(r\"#(\\w+)\", tweet)\n",
        "  return hash\n",
        "\n",
        "def remove_username(tweet):\n",
        "  text = re.sub(r\"@[A-Za-z]+[A-Za-z0-9-_]+\", \"\", tweet)\n",
        "  return text\n",
        "\n",
        "def remove_links(tweet):\n",
        "  text = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "  text = text.strip(\"[link]\")\n",
        "  return text\n",
        "\n",
        "def remove_non_ascii(x):    # only use if you do not plan to translate tweets to other languages\n",
        "  return \"\".join(i for i in x if ord(x) < 128)\n",
        "\n",
        "def lowercase_tweet(tweet):\n",
        "  return tweet.lower()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cunPvwyaCgPD",
        "outputId": "847aed1b-27ff-484a-b500-aa14830b769f"
      },
      "source": [
        "sample_tweet = \"this is #fun! let's #doThis more often @sid! https.google.com\"\n",
        "\n",
        "print(remove_username(sample_tweet))\n",
        "print(hashtags(sample_tweet))\n",
        "print(remove_links(sample_tweet))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is #fun! let's #doThis more often ! https.google.com\n",
            "['fun', 'doThis']\n",
            "this is #fun! let's #doThis more often @sid! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnmu5xV1mzML"
      },
      "source": [
        "# Stopwords removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnf4J6xsOO44"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stops = set(stopwords.words('english'))\n",
        "  # you can add custom stopwords as well using stops.update()\n",
        "  stops.update((\"brb\", \"yolo\", \"hodl\", \"it\", \"this\", \"mailto\"))\n",
        "  new_text = \" \".join(word for word in text.split() if word not in stops)\n",
        "  return new_text\n",
        "  "
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UGwkec6InM_7",
        "outputId": "c0e443bd-e7e2-4159-a1ce-5138c225af35"
      },
      "source": [
        "sample_text = \"bitcoin is goin down. But i am gonna hodl coz yolo\"\n",
        "\n",
        "remove_stopwords(sample_text)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bitcoin goin down. But gonna coz'"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6s37qCVq7Tt"
      },
      "source": [
        "# Named Entity Recognition (NER) ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T13KpqTYomdV"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIWP2Xun9eDT",
        "outputId": "92045eef-ebfd-49e6-f50b-1d5736e3994c"
      },
      "source": [
        "text_sample = \"The cryptocurrency market was back in the green on Tuesday as investors picked quality tokens at lower prices. Bitcoin hit the $50,000 mark as concerns over the Omicron variant eased across the globe and investors lapped up riskier assets.\"\n",
        "doc = nlp(text_sample)\n",
        "ner = [(x.text, x.label_) for x in doc.ents] \n",
        "ner"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Tuesday', 'DATE'),\n",
              " ('Bitcoin', 'GPE'),\n",
              " ('$50,000 mark', 'MONEY'),\n",
              " ('Omicron', 'ORG')]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ofxg2V5_UeY"
      },
      "source": [
        ""
      ]
    }
  ]
}